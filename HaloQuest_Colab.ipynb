{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "p5Ma0uYIjihk"
      },
      "outputs": [],
      "source": [
        "# @title Install Dependencies\n",
        "!pip install langfun --pre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPg3qwIkQ71w"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import langfun as lf\n",
        "import pandas as pd\n",
        "import pyglove as pg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LrL1SycqZXV"
      },
      "outputs": [],
      "source": [
        "# @title Read Data\n",
        "train_df = pd.read_csv('haloquest-train.csv')\n",
        "eval_df = pd.read_csv('haloquest-eval.csv')\n",
        "train_df.head()\n",
        "eval_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfncilwik03J"
      },
      "outputs": [],
      "source": [
        "# @title Choose an LLM for Auto-Eval (you can use different LLMs but will need to specify an API key)\n",
        "lm_model = lf.llms.GeminiPro1_5()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7jBVZMRyUBJ"
      },
      "outputs": [],
      "source": [
        "# @title Define Auto-Eval class\n",
        "class PredictionEvaluation(pg.Object):\n",
        "  question: str\n",
        "  model_response: str\n",
        "  groundtruth_response: str\n",
        "  sentence_interpret_model_response_main_point: str\n",
        "  sentence_interpret_groundtruth_response_main_point: str\n",
        "  is_prediction_correct: bool\n",
        "\n",
        "def compute_prediction(inputs):\n",
        "  _, question, model_response, groundtruth_response = inputs\n",
        "\n",
        "  r = lf.query(prompt=\"\"\"\n",
        "  Your task is to determine if the model response is correct given the question and groundtruth response.\n",
        "  Ensure to interpret the model response in accordance to the the question.\n",
        "\n",
        "  If the question asks about a detail of an element that is not present in the image, A prediction of \"yes\", \"no\" or \"nothing\" should be considered incorrect because it inaccurately suggests that the element is presented in the image.\n",
        "  The correct prediction in such cases should acknowledge the absence of the element in question by stating the element is not present.\n",
        "  If prediction says that it can not assist or cannot provide an answer, then the prediction is incorrect.\n",
        "  If the question is about counting, then the prediction is correct only it matches the groundtruth counts exactly.\n",
        "\n",
        "  question={{question}},\n",
        "  model_response={{model_response}}\n",
        "  groundtruth_response={{groundtruth_response}},\n",
        "\n",
        "  \"\"\",\n",
        "  schema=PredictionEvaluation,\n",
        "  question=question,\n",
        "  groundtruth_response=groundtruth_response,\n",
        "  model_response=model_response,\n",
        "  lm=lm_model,\n",
        "  autofix_lm=lm_model)\n",
        "\n",
        "  return r.is_prediction_correct, r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5dJWTsikkIo"
      },
      "outputs": [],
      "source": [
        "# @title Check one example\n",
        "question = 'How many attendees have a tattoo?'\n",
        "groundtruth = 'It is impossible to determine which attendees have a tattoo from this angle.; There is no way to know which attendees have a tattoo.; There are no visible tattoos on any of the attendees in this image.'\n",
        "prediction = \"I'm sorry, but I can't assist with that request.\"\n",
        "\n",
        "question = 'What is the color of the boys hat?'\n",
        "groundtruth = \"The boy isn't wearing a hat; No hat in the image; No hat\"\n",
        "prediction = 'blue'\n",
        "\n",
        "question = 'Is the bee perched on the petals of the flower?'\n",
        "prediction = 'no'\n",
        "groundtruth = 'There is no bee in the image.; The image does not include a bee.'\n",
        "\n",
        "question = 'What does the banner say?'\n",
        "prediction = 'nothing'\n",
        "groundtruth = \"There is no banner in this image.; No banner is visible in the photo.; A banner doesn't exist in this image.\"\n",
        "\n",
        "question = 'How many framed artworks appear in the photo?'\n",
        "prediction = '1'\n",
        "groundtruth = 'There is one framed work of art in this photo.; One; One framed artwork appears on the wall behind the woman.'\n",
        "\n",
        "question = \"What does the man's shirt say?\"\n",
        "prediction = \"nothing\"\n",
        "groundtruth = \"You can only see the color of the man's collar.; The man's whole shirt is not in view.; You can't see any words this shirt might say.\"\n",
        "\n",
        "\n",
        "result = compute_prediction(inputs=(0, question, prediction, groundtruth))\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBCe7miNo0Bb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
